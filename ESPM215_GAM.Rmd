---
title: "ESPM 215 - Generalized Additive Models (GAMs)"
author: "Dorothy Chen, Kenzo Esquivel, Kane Russell, and Yvonne Socolar"
date: "2/25/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
library(mgcv)
library(nlme)
library(tidyverse)
library(gamm4)
```

```{r}
ISIT <- read.delim('ISIT.txt') # bioluminescence dataset (Zuur Ch. 3, 17)
GF <- read.csv('jpe12720-sup-0003-apps2-greenfinch.csv') # green finch count data, Knape 2016
GC <- read.csv('jpe12720-sup-0002-apps2-goldcrest.csv') # gold crest count data, Knape 2016

ISIT$fMonth <- factor(ISIT$Month)
ISIT$fStation <- factor(ISIT$Station)
ISIT$fYear <- factor(ISIT$Year)
ISIT2 <- ISIT[ISIT$fStation != "4" &
              ISIT$fStation != "5" &
              ISIT$fStation != "10" ,]
ISIT2$Depth1000 <- ISIT2$SampleDepth/1000
```

# Comparing GLMs to GAMs

```{r, echo = TRUE}

glmfit1 <- glm(Sources ~ fStation + Depth1000 + fMonth, 
               data = ISIT2, family = gaussian(link = "identity"))
par(mfrow = c(2, 2))
plot(glmfit1)
summary(glmfit1)

gamfit1 <- gam(Sources ~ fStation + s(Depth1000) + fMonth, 
               data = ISIT2)
par(mfrow = c(1, 1))
plot(gamfit1)
gam.check(gamfit1)
summary(gamfit1)
```

## explain single variable GAM output

## Visually choose optimal lambda
```{r}
lambda <- 1 ### try different values of lambda
ISIT16 <- filter(ISIT2, fStation == 16)
fit1 <- gam(Sources ~ s(Depth1000, sp = lambda), data = ISIT16)
depth_seq <- data.frame(Depth1000 = seq(min(ISIT16$Depth1000), max(ISIT16$Depth1000), length = 100))
pred <- predict.gam(fit1, depth_seq)

ggplot() + 
  geom_point(aes(Depth1000, Sources), ISIT16) + 
  geom_line(aes(depth_seq$Depth1000, pred), color = 'red')
```

## Create lambda vs. GCV graph to choose optimal lambda
```{r}
lambda <- ### try a range of lambda values
GCV <- 0

for (i in 1:length(lambda)) {
  gam_fit <- gam(Sources ~ s(Depth1000, sp = lambda[i]), data = ISIT16) 
  GCV[i] <- gam_fit$gcv.ubre
}

ggplot(data.frame(lambda, GCV)) +
  aes(lambda, GCV) + 
  geom_line()
```

## Compare with automatic GCV from model fit
```{r}
fit2 <- gam(Sources ~ s(Depth1000), data = ISIT16)
summary(fit2)

fit2$sp # smoothing parameter lambda
fit2$gcv.ubre # GCV score
```

## Incorporating random effects with Knape data

```{r}
# using the gam() function (mgcv package)
GC$fyr <- factor(GC$yr)
GC_fit1 <- gam(count ~ s(fyr, bs = 're', k = 8) + 
                    #s(site, bs = 're') + # adding this term slows down gam() a lot!
                    s(latitude) + 
                    s(observerAge, k = 20) + 
                    s(day) + 
                    firstSurvey + 
                    offset(log(lineCov)), 
            family = quasipoisson(), 
            data = GC)

summary(GC_fit1)
```

```{r}
# using the gamm() function (mgcv package)
GC$loglineCov <- log(GC$lineCov)
GC_fit2 <- gamm(count ~ offset(loglineCov) + 
                        s(latitude) + 
                        s(observerAge, k = 20) + 
                        s(day) + 
                        firstSurvey, 
                random = list(site =~ 1, fyr =~ 1),
                family = quasipoisson(), 
                data = GC)

summary(GC_fit2$gam)
summary(GC_fit2$lme)
```

```{r}
# using the gamm4 function (gamm4 package)
GC_fit3 <- gamm4(count ~ offset(log(lineCov)) + 
                        s(latitude) + 
                        s(observerAge, k = 20) + 
                        s(day) + 
                        firstSurvey, 
                random = ~ (1|site) + (1|fyr),
                family = quasipoisson(), 
                data = GC)

summary(GC_fit2$gam)
summary(GC_fit2$mer)
```

## Discussion on paper
- What are the main takeaways of this paper?
- How did the author use GAMMs to evaluate temporal trends in the bird count data?
- What were the effects and implications of using automatic vs. fixed degrees of freedom?


### Different packages for GAM processing. 

There are a number of different packages for GAM in R. 
  - the *gam* package was written by Hastie and Tibshirani (credited for introducing GAMs) and estimates the smoothing functions using a back-fitting algorithm (which allows for estimation of one smoother at a time)   \
  - the *mgcv* package, standing for "Mixed GAM Computation Vehicle with Automatic Smoothness Estimation", was introduced by Wood, and uses splines (which reduces function approximation error for a given dimension of smoothing basis). This package can handle large datasets, do cross validation, and allow for expansions into Generalized Additive Mixed Modeling (GAMM) frameworks. GAMM fitting relies on the nmle package, which is great for normally distributed models but not suitable for Poisson or Binomial GLMMs. A detailed walkthrough can be found at [this site](https://people.maths.bris.ac.uk/~sw15190/mgcv/tampere/mgcv.pdf)    \
  - the *gamm4* package, which is similar to the *mgcv* package but relies on the lme4 package for calculations (Fabian Scheipl trick)  \


# Single value GAM fitting and talking about the R output 

$sources_{i} = \alpha + f(Depth_{i}) + factor(Station_{i}) + \epsilon_{i}$

$ \epsilon_{i} \sim N(0, \sigma^{2})$

## Show model fitting with different values of lambda

### INclude observed values on the fitted line 

=======



# More complex spline fitting and incorporating multiple variables with different smoothing techniques 

















