---
title: "ESPM 215 - Generalized Additive Models (GAMs)"
author: "Dorothy Chen, Kenzo Esquivel, Kane Russell, and Yvonne Socolar"
date: "2/25/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
library(mgcv)
library(nlme)
library(tidyverse)
library(gamm4)
```

```{r}
ISIT <- read.delim('ISIT.txt') # bioluminescence dataset (Zuur Ch. 3, 17)
GF <- read.csv('jpe12720-sup-0003-apps2-greenfinch.csv') # green finch count data, Knape 2016
GC <- read.csv('jpe12720-sup-0002-apps2-goldcrest.csv') # gold crest count data, Knape 2016

ISIT$fMonth <- factor(ISIT$Month)
ISIT$fStation <- factor(ISIT$Station)
ISIT$fYear <- factor(ISIT$Year)
ISIT2 <- ISIT[ISIT$fStation != "4" &
              ISIT$fStation != "5" &
              ISIT$fStation != "10" ,]
ISIT2$Depth1000 <- ISIT2$SampleDepth/1000
```

# Comparing GLMs to GAMs

```{r, echo = TRUE}

glmfit1 <- glm(Sources ~ Depth1000, 
               data = ISIT2, family = gaussian(link = "identity"))

gamfit1 <- gam(Sources ~ s(Depth1000), 
               data = ISIT2)

glmfit2 <- glm(Sources ~ Depth1000, 
               data = ISIT2, family = quasipoisson((link = "log")))

gamfit2 <- gam(Sources ~ s(Depth1000), 
               data = ISIT2,family = quasipoisson((link = "log")))

# Normal GLM
par(mfrow = c(1, 1))
plot(ISIT2$Depth1000, ISIT2$Sources)

points(ISIT2$Depth1000, fitted(glmfit1), col=2,pch="+") #red

# Poisson GLM
points(ISIT2$Depth1000, fitted(glmfit2), col=3,pch="+") #green

# GAM 
points(ISIT2$Depth1000, fitted(gamfit1), col = 4, pch = "*") #blue

# Plot of GAM spline term
mgcv::plot.gam(gamfit1, se = T) # Plot of smooth function on the scale of the linear predictor

# Examine residuals
par(mfrow = c(2, 2))
plot(glmfit1) 
plot(glmfit2)
gam.check(gamfit1)

par(mfrow = c(1, 1))
boxplot(glmfit1$residuals,gamfit1$residuals, glmfit2$residuals, gamfit2$residuals,
            names=c("GLM 1","GAM1","GLM 2", "GAM2"))

# GLM output
summary(glmfit1)

# GAM output
summary(gamfit1)

# AIC comparison
AIC(glmfit1, gamfit1)

```

## Notes about interpreting GAM output 
  - **edf**: effective degrees of freedom corresponds to the number of parameters left when corresponding parameters have been penalized to some extent (through smoothing process and penalized regression estimation). edf = 1 for simple linear relationship; range is [1, k - 1], where k value depends on the basis function
  - p-values: p-values should be interpeted with caution
  - $r^{2}$ value: can be used for standard Gaussian setting, adjusted $r^{2}$ = "deviance explained"
  - GCV = generalized cross validation score = mean of estimated MSE based on LOOCV

## Visually choose optimal lambda
```{r}
lambda <- 0.00025 ### try different values of lambda
ISIT16 <- filter(ISIT2, fStation == 16)
fit1 <- gam(Sources ~ s(Depth1000, sp = lambda), data = ISIT16)
depth_seq <- data.frame(Depth1000 = seq(min(ISIT16$Depth1000), max(ISIT16$Depth1000), length = 100))
pred <- predict.gam(fit1, depth_seq)

ggplot() + 
  geom_point(aes(Depth1000, Sources), ISIT16) + 
  geom_line(aes(depth_seq$Depth1000, pred), color = 'red')
```

## Create lambda vs. GCV graph to choose optimal lambda
```{r}
lambda <- seq(0, 0.001, by = 0.00001) ### try a range of lambda values
GCV <- 0

for (i in 1:length(lambda)) {
  gam_fit <- gam(Sources ~ s(Depth1000, sp = lambda[i]), data = ISIT16) 
  GCV[i] <- gam_fit$gcv.ubre
}

ggplot(data.frame(lambda, GCV)) +
  aes(lambda, GCV) + 
  geom_line()
```

## Compare with automatic GCV from model fit
```{r}
fit2 <- gam(Sources ~ s(Depth1000), data = ISIT16)
summary(fit2)

fit2$sp # smoothing parameter lambda
fit2$gcv.ubre # GCV score
```

## Incorporating random effects with Knape data

```{r}
# using the gam() function (mgcv package)
GC$fyr <- factor(GC$yr)
GC_fit1 <- gam(count ~ s(fyr, bs = 're', k = 8) + 
                    #s(site, bs = 're') + # adding this term slows down gam() a lot!
                    s(latitude) + 
                    s(observerAge, k = 20) + 
                    s(day) + 
                    firstSurvey + 
                    offset(log(lineCov)), 
            family = quasipoisson(), 
            data = GC)

summary(GC_fit1)
```

```{r}
# using the gamm() function (mgcv package)
GC$loglineCov <- log(GC$lineCov)
GC_fit2 <- gamm(count ~ offset(loglineCov) + 
                        s(latitude) + 
                        s(observerAge, k = 20) + 
                        s(day) + 
                        firstSurvey, 
                random = list(fyr =~ 1),# random effect, has to be in list form
                family = quasipoisson(), 
                data = GC)

summary(GC_fit2$gam) 
summary(GC_fit2$lme)


```

```{r}
# using the gamm4 function (gamm4 package)
GC_fit3 <- gamm4(count ~ offset(log(lineCov)) + 
                        s(latitude) + 
                        s(observerAge, k = 20) + 
                        s(day) + 
                        firstSurvey, 
                random = ~ (1|site) + (1|fyr),
                family = quasipoisson(), 
                data = GC)

summary(GC_fit3$gam)
summary(GC_fit3$mer)
```

## Discussion on paper
- What are the main takeaways of this paper?
- How did the author use GAMMs to evaluate temporal trends in the bird count data?
- What were the effects and implications of using automatic vs. fixed degrees of freedom?

=======

## Discussion on paper
- What are the main takeaways of this paper?
- How did the author use GAMMs to evaluate temporal trends in the bird count data?
- What were the effects and implications of using automatic vs. fixed degrees of freedom?

=======

# Different packages for GAM processing. 

There are a number of different packages for GAM in R. 
  - the *gam* package was written by Hastie and Tibshirani (credited for introducing GAMs) and estimates the smoothing functions using a back-fitting algorithm (which allows for estimation of one smoother at a time)   \
  - the *mgcv* package, standing for "Mixed GAM Computation Vehicle with Automatic Smoothness Estimation", was introduced by Wood, and uses splines (which reduces function approximation error for a given dimension of smoothing basis). This package can handle large datasets, do cross validation, and allow for expansions into Generalized Additive Mixed Modeling (GAMM) frameworks. GAMM fitting relies on the nmle package, which is great for normally distributed models but not suitable for Poisson or Binomial GLMMs. A detailed walkthrough can be found at [this site](https://people.maths.bris.ac.uk/~sw15190/mgcv/tampere/mgcv.pdf)    \
  - the *gamm4* package, which is similar to the *mgcv* package but relies on the lme4 package for calculations (Fabian Scheipl trick)  \

# Other reference material online

Michael Clark's [GAM vignette](https://m-clark.github.io/generalized-additive-models/technical.html) provides a more extensive discussion of all things GAM. 

MSU also put together an [R-lab on GAMs](http://ecology.msu.montana.edu/labdsv/R/labs/lab5/lab5.html). 

Jacolien van Rij wrote a vignette on [visualization of GAMM models](https://cran.r-project.org/web/packages/itsadug/vignettes/inspect.html). 







